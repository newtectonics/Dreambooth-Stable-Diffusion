{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the ldm environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    !pip install albumentations==1.1.0\n",
    "    !pip install opencv-python>=4.2.0.34\n",
    "    !pip install pudb==2019.2\n",
    "    !pip install imageio==2.14.1\n",
    "    !pip install imageio-ffmpeg==0.4.7\n",
    "    !pip install pytorch-lightning==1.5.9\n",
    "    !pip install omegaconf==2.1.1\n",
    "    !pip install test-tube>=0.7.5\n",
    "    !pip install streamlit>=0.73.1\n",
    "    !pip install setuptools==59.5.0\n",
    "    !pip install pillow==9.0.1\n",
    "    !pip install einops==0.4.1\n",
    "    !pip install torch-fidelity==0.3.0\n",
    "    !pip install transformers==4.18.0\n",
    "    !pip install torchmetrics==0.6.0\n",
    "    !pip install kornia==0.6\n",
    "    !pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "    !pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get the base model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
